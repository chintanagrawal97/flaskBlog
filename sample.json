{"Keyword": [{"/Users/achintan/Desktop/emr/j-100/containers/app3/container_1588014112959_0014_01_000002/stderr.gz": []}], "KeywordLen": 108, "SpecificError": [{"ERROR TaskSetManager": {"/Users/achintan/Desktop/emr/j-100/containers/app3/container_1588014112959_0014_01_000002/stderr.gz": ["20/04/27 22:52:36 ERROR TaskSetManager: Task 12 in stage 39.0 failed 4 times; aborting job \t "]}}, {"Job aborted due to stage failure": {}}, {"ERROR Executor": {}}, {"ERROR ApplicationMaster": {}}, {"Broken pipejava.io.IOException": {}}, {"Executor self-exiting due to : Driver": {}}, {"ERROR TransportResponseHandler": {}}, {"ERROR TransportClient": {}}, {"ERROR RetryingBlockFetcher: Exception while beginning fetch": {}}, {"ERROR Utils": {}}, {"ERROR SparkHadoopWriter: Aborting job": {}}, {"OneForOneBlockFetcher: Failed while starting block fetchesjava.io.IOException": {}}, {"ERROR RpcOutboxMessage": {}}, {"ERROR SparkContext: Error initializing SparkContext": {}}], "SpecificWarn": [{"exception org.apache.spark.TaskKilledException": {}}, {"failed due to exception java.lang.IllegalStateException": {}}, {"could not be removed as it was not found on disk or in memory": {}}, {"OneWayOutboxMessage: Failed to send one-way RPC": {}}, {"WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC": {}}, {"Failed to fetch remote block": {}}, {"Exception in connection": {}}, {"WARN YarnAllocator: Container killed by YARN for exceeding memory limits": {}}, {"failed due to exception org.apache.spark.SparkException: No port number": {}}, {"NioEventLoop: Selector.select() returned prematurely": {}}, {"exception org.apache.spark.SparkException: Exception thrown in awaitResult": {}}, {"WARN YarnAllocator: Container from a bad node": {}}, {"Issue communicating with driver in heartbeaterorg.apache.spark.SparkException": {}}, {"WARN SparkConf: spark.master yarn-cluster is deprecated in Spark 2.0+": {}}, {"Unable to eagerly init filesystem s3://does/not/exist": {}}, {"SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated": {}}, {"WARN Executor: Issue communicating with driver in heartbeater": {}}], "filepath": " ", "flag": 0, "res": [{"/Users/achintan/Desktop/emr/j-100/containers/app3/container_1588014112959_0014_01_000002/stderr.gz": {"ERROR": ["20/04/27 22:52:36 ERROR TaskSetManager: Task 12 in stage 39.0 failed 4 times; aborting job \t ", "20/04/27 22:52:36 ERROR FileFormatWriter: Aborting job e0160879-0090-489e-ba08-a387dcd1e82f. \t ", "20/04/27 22:52:36 ERROR HiveUtil: An Error Occured in Method insertIntoHiveTable in Class HiveUtil with Error Message - Job aborted. - FOR TABLE NAME :BOMBOD_ASPROVIDED.PRTBILL \t org.apache.spark.SparkException: Job aborted. \t \tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198) \t \tat org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.saveAsHiveFile(SaveAsHiveFile.scala:86) \t \tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:66) \t \tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:195) \t \tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:99) \t \tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104) \t \tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102) \t \tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122) \t \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) \t \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) \t \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) \t \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) \t "], "INFO": [], "WARN": []}}], "resLen": 1798}